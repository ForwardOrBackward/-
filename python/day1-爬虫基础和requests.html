<html>
<head>
  <title>day1-爬虫基础和requests</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/603932 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 12pt;
    }
  </style>
</head>
<body>
<a name="1086"/>
<h1>day1-爬虫基础和requests</h1>
<div>
<table bgcolor="#D4DDE5" border="0">
<tr><td><b>创建时间：</b></td><td><i>2019/7/23 15:14</i></td></tr>
<tr><td><b>更新时间：</b></td><td><i>2020/7/16 11:33</i></td></tr>
</table>
</div>
<br/>

<div>
<span><div><span style="font-weight: bold;">1、爬虫的原理</span></div><div>（1）概述</div><div><img src="day1-爬虫基础和requests_files/Image.png" type="image/png" data-filename="Image.png"/></div><div> (2)  判断一个“ . ”后面的单词是属性还是函数（方法）的小技巧：</div><div>        主要根据词性去判断，动词一般就是方法，而名词一般就是属性(并且方法有括号,属性没括号)</div><div>        如requests.get() ，get很明显是一个动词，所以它肯定是一个方法，所以要加括号</div><div>        再如requests.get().content.decode() ,其实就相当于</div><div>        response=requests.get() ;   response.content.decode()</div><div>        可以看出，content是名词，所以它是一个属性，不用跟括号，而的decode是动词，所以是一个方法，后面要加括号</div><div><br/></div><div>（3）response.text 和response.content的区别</div><div>        更推荐使用response.content.deocde()的方式获取响应的html页面</div><div>  response.text</div><div>    • 类型：str</div><div>    • 解码类型： 根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码    <span style="color: rgb(255, 0, 0);">#但有时候系统推测出来的编</span></div><div><span style="color: rgb(255, 0, 0);">                码方式很多时候都是错的</span><span style="color: rgb(255, 0, 0);">，所以这方法一般最后实在猜不出来编码方式才使用，才让系统来判断。这个时</span></div><div><span style="color: rgb(255, 0, 0);">                候系统的判断一般是比人要准确一些</span></div><div>• 如何修改编码方式：response.encoding=”gbk”</div><div> response.content</div><div>    • 类型：<span style="color: rgb(255, 0, 0);">bytes                    bytes类型的数据是二进制类型的数据，是一种没有经过任何修改的原汁原味的数据</span></div><div>    • 解码类型： 没有指定</div><div>    • 如何修改编码方式：response.content.deocde(“gbk”) <span style="color: rgb(255, 70, 53);"> #在python3中</span><span style="color: rgb(255, 70, 53);">response.content.deocde（）方</span></div><div><span style="color: rgb(255, 70, 53);">                法的默认解码方式就是utf-8，而大部分网页采取的编码方式方式就是utf-8，所以更推荐用</span></div><div><span style="color: rgb(255, 70, 53);">             </span> <span style="color: rgb(255, 70, 53);">  </span><span style="color: rgb(255, 70, 53);">response.content.deocde（）方法来获得响应的html页面。若网页不是utf-8编码的，就很有可能是</span></div><div><span style="color: rgb(255, 70, 53);">                    gbk了，此时只要在括号里面填入gbk就可以，选择如何解码的优先顺序如</span><span style="color: rgb(255, 70, 53);">下</span><span style="color: rgb(255, 70, 53);">图所示：</span></div><div><img src="day1-爬虫基础和requests_files/Image [1].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div><span style="font-weight: bold;">2、HTTP常用请求头</span></div><div>（1）请求头的格式如下：</div><div><span style="font-weight: bold;"><img src="day1-爬虫基础和requests_files/Image [2].png" type="image/png" data-filename="Image.png"/></span></div><div><br/></div><div>（2）各头部字段的含义：</div><div><span style="font-weight: bold;"><img src="day1-爬虫基础和requests_files/Image [3].png" type="image/png" data-filename="Image.png"/></span></div><div><span style="font-weight: bold;"><img src="day1-爬虫基础和requests_files/Image [4].png" type="image/png" data-filename="Image.png" width="916"/></span></div><div><span style="color: rgb(255, 0, 0);">1. Host (主机和端口号)</span></div><div><span style="color: rgb(255, 0, 0);">2. Connection (链接类型)(keep-alive表示常连接，也就是浏览器告诉服务器，自己支持复用上次的连接，支持使</span></div><div><span style="color: rgb(255, 0, 0);">                            用常连接，也就是一段时间内保持与该页面的连接，这样当短时间内再次访问该url时，不用再经</span></div><div><span style="color: rgb(255, 0, 0);">                            过三次握手四次挥手，有利于提高访问速度和效率）</span></div><div><span style="color: rgb(255, 0, 0);">3. Upgrade-Insecure-Requests (是否会升级为HTTPS请求，</span>直译过来是：是否升级不安全的请求，0表示否，1</div><div>                            表示是<span style="color: rgb(255, 0, 0);">)（比如若值为1，则但url写成 </span><span style="color: rgb(166, 0, 196);">http</span><span style="color: rgb(255, 0, 0);">://www.baidu.com时，会在访问改url前自动转换为</span></div><div><span style="color: rgb(255, 0, 0);">    </span><span style="color: rgb(255, 0, 0);">    </span><span style="color: rgb(255, 0, 0);">    </span><span style="color: rgb(255, 0, 0);">    </span><span style="color: rgb(255, 0, 0);">    </span><span style="color: rgb(255, 0, 0);">    </span><span style="color: rgb(255, 0, 0);">    </span><span style="color: rgb(255, 0, 0);">  </span> <span style="color: rgb(166, 0, 196);"> </span><a href="http://www.baidu.com/" style="color: rgb(166, 0, 196);">https</a><a href="http://www.baidu.com/" style="color: rgb(255, 0, 0);">://www.baidu.com</a><span style="color: rgb(255, 0, 0);"> </span><span style="color: rgb(255, 0, 0);">）</span></div><div><span style="color: rgb(255, 0, 0);">4. User-Agent (浏览器标识，也叫用户代理) （指明用户用的是什么浏览器，什么操作系统，</span></div><div><span style="color: rgb(255, 0, 0);">            如</span> <span style="font-size: 12px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(34, 34, 34); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace; font-variant-caps: normal; font-variant-ligatures: normal;">Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36</span></div><div><span style="font-size: 12px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(34, 34, 34); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace; font-variant-caps: normal; font-variant-ligatures: normal;">           Mozilla/5.0</span><span style="font-size: 14pt; color: rgb(255, 0, 0);"> </span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14pt; color: rgb(255, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">用来说明和Mozilla渲染引擎的兼容性</span><span style="color: rgb(255, 0, 0);">，后面紧跟的</span><span style="font-size: 12px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(34, 34, 34); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace; font-variant-caps: normal; font-variant-ligatures: normal;">(Windows NT 10.0; WOW64</span><span style="font-size: 12px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(34, 34, 34); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace; font-variant-caps: normal; font-variant-ligatures: normal;">)</span><span style="color: rgb(255, 0, 0);">是当前电脑的信息， </span></div><div><span style="color: rgb(255, 0, 0);">               </span><span style="font-size: 12px; white-space: pre-wrap; color: rgb(34, 34, 34); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace;">AppleWebKit/537.36</span><span style="color: rgb(255, 0, 0);">是苹果浏览器的内核版本，</span><span style="font-size: 12px; white-space: pre-wrap; color: rgb(34, 34, 34); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace;">Chrome/69.0.3497.100</span><span style="white-space: pre-wrap;"><font style="font-size: 14pt;"><span style="font-size: 14pt; color: rgb(255, 0, 0); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace;">是Chrome</span></font></span><span style="color: rgb(255, 0, 0);">浏览器的内核版本</span><span style="white-space: pre-wrap;"><font style="font-size: 14pt;"><span style="font-size: 14pt; color: rgb(227, 0, 0); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace;">，</span></font></span><span style="font-size: 12px; white-space: pre-wrap; color: rgb(34, 34, 34); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace;">Safari/537.36</span><span style="white-space: pre-wrap;"><font style="font-size: 14pt;"><span style="font-size: 14pt; color: rgb(227, 0, 0); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace;">是</span></font></span></div><div><span style="white-space: pre-wrap;"><font style="font-size: 14pt;"><span style="font-size: 14pt; color: rgb(227, 0, 0); font-family: Consolas, &quot;Lucida Console&quot;, &quot;Courier New&quot;, monospace;">          Safari</span></font></span><span style="color: rgb(255, 0, 0);">浏览器的内核版本</span><span style="color: rgb(227, 0, 0);">）</span></div><div><span style="color: rgb(255, 0, 0);">5. Accept (传输文件类型) （告诉服务端支持什么样类型的数据）</span></div><div><span style="color: rgb(255, 0, 0);">6. Referer (页面跳转处)（记录当前页是从哪个页面跳转过来的。比如在&quot;百度知道</span><span style="color: rgb(255, 0, 0);">&quot;</span><span style="color: rgb(255, 0, 0);">页面点了一个按钮，直接在当前</span></div><div><span style="color: rgb(255, 0, 0);">                    页(不另开一页)跳转到百度首页，则Referer记录当前页是从zhidao.baidu.com跳转过来的）</span></div><div><span style="color: rgb(255, 0, 0);">7. Accept-Encoding（文件编解码格式） （浏览器告诉服务端自己会接受什么样的页面压缩方式）</span></div><div><span style="color: rgb(255, 0, 0);">8.Accept-Language （浏览器接受的语言）（CN表示Chinese，中文）</span></div><div><span style="color: rgb(255, 0, 0);">9. x-requested-with :XMLHttpRequest  (Ajax 异步请求)</span></div><div><span style="color: rgb(255, 0, 0);">10. Cookie （Cookie）</span></div><div><span style="color: rgb(255, 0, 0);">    </span><span style="color: rgb(255, 0, 0);">   </span>(1) Cookie和Session的区别：Cookie是保存在浏览器本地的，Session是保存在服务器上面的。Cookie由于</div><div>                保存在浏览器本地，所以可以直接看见Cookie的内容，所以Cookie往往是不安全的；而Session保存在服</div><div>                务器上，一般用户无法获取到其内容，所以Session往往是安全的<img src="day1-爬虫基础和requests_files/Image [5].png" type="image/png" data-filename="Image.png"/></div><div>        (2)Cookie在请求头里的内容如下图所示：</div><div><img src="day1-爬虫基础和requests_files/Image [6].png" type="image/png" data-filename="Image.png"/></div><div>        Cookie表的内容如下图所示：（表里的记录不齐全，还有很多记录没截进来，这只是一部分）</div><div><img src="day1-爬虫基础和requests_files/Image [7].png" type="image/png" data-filename="Image.png"/></div><div>        可以看出请求头里的内容和Cookie表的Name属性和Value属性一一对应，用等于号“=”将二者连接，并用分号隔开一条条记录。而Cookie表除了Name和Value属性，其他属性在请求头里并没有什么用。</div><div><br/></div><div><br/></div><div><span style="font-weight: bold;">3、</span><span style="font-weight: bold;">url的形式</span></div><div>形式 ：<a href="scheme://host/">scheme://host</a>[:port#]/path/…/[?query-string][#anchor]</div><div><br/></div><div>scheme：协议 (例如：http, https, ftp)</div><div>host：服务器的<span style="color: rgb(166, 0, 196);">IP地址或者域名</span> （所以说也可以直接写IP地址）</div><div>port：服务器的端口（如果是走协议默认端口，80 or 443）</div><div>path：访问资源的路径</div><div>query-string：参数，发送给http服务器的数据</div><div>anchor：锚（跳转到网页的指定锚点位置）（如 <a href="http://item.jd.com/11936238.html#product-detail">http://item.jd.com/11936238.html</a><a href="http://item.jd.com/11936238.html#product-detail" style="color: rgb(255, 0, 0);">#product-detail</a> ，将在页</div><div>                        面渲染出来后直接跳转到该URL页面中的product-detail部分）</div><div><br/></div><div><span style="font-weight: bold;">4、Python3中的两种字符串类型</span></div><div>• bytes：二进制</div><div>                互联网上数据的都是以二进制的方式传输的，所以从互联网上爬取的响应内容都是二进制的形式，所以</div><div>                    如果我们想看一下响应的内容的话，第一件事就是进行解码：decode（）</div><div>• str ：unicode的呈现形式</div><div>        ASCII编码是1个字节，而Unicode编码通常是2个字节。</div><div>        UTF-8是Unicode的实现方式之一，UTF-8是它是一种变长的编码方式，可以是1，2，3个字节</div><div><br/></div><div><span style="font-weight: bold;">5、request请求</span></div><div>（1）状态码</div><div><span style="font-weight: bold;"><img src="day1-爬虫基础和requests_files/Image [8].png" type="image/png" data-filename="Image.png"/></span></div><div><span style="font-weight: bold;"><img src="day1-爬虫基础和requests_files/Image [9].png" type="image/png" data-filename="Image.png"/></span></div><div>（2）其他用法</div><div><img src="day1-爬虫基础和requests_files/Image [10].png" type="image/png" data-filename="Image.png"/></div><div><img src="day1-爬虫基础和requests_files/Image [11].png" type="image/png" data-filename="Image.png"/></div><div><img src="day1-爬虫基础和requests_files/Image [12].png" type="image/png" data-filename="Image.png"/></div><div><span style="font-weight: bold;">6、发送带参数的请求</span></div><div>    什么叫做请求参数：</div><div>    例： <a href="https://www.baidu.com/s?wd=python&amp;c=b">https://www.baidu.com/s</a><a href="https://www.baidu.com/s?wd=python&amp;c=b" style="color: rgb(77, 206, 29);">?</a><a href="https://www.baidu.com/s?wd=python&amp;c=b" style="color: rgb(255, 0, 0);">wd=python&amp;c=b</a></div><div>            wd就是参数名，python是它的值，参数与参数之间用“&amp;”符号连接。</div><div>    • 参数的形式：字典</div><div>    • kw = {'wd':'长城'}</div><div>    • 用法：requests.get(url,params=kw)   </div><div>                        记得此时的url要带上参数前面的所有字符，也就是url要写成<a href="https://www.baidu.com/s?wd=python&amp;c=b">https://www.baidu.com/s<font color="#FF0000">?</font></a></div><div>                        所以其实加上参数也就是相当于url与params的一个直接拼接而已</div><div>所以，既然params只是一个简单的url拼接，那么就有了<span style="color: rgb(255, 0, 0);">更简单的方法，直接用其他方法来生成url，而不使用url和参数分开这种更费力的方法。</span></div><div><img src="day1-爬虫基础和requests_files/Image [13].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div><span style="font-weight: bold;">7、requests模块发送post请求</span></div><div>（1）发送post请求常见的情况是</div><div>    • 登录注册（ POST 比 GET 更安全）</div><div>    • 需要传输大文本内容的时候（ POST 请求对数据长度没有要求）</div><div>（2）用法：</div><div>    response = requests.<span style="color: rgb(255, 0, 0);">post</span>(&quot;<a href="http://www.baidu.com/">http://www.baidu.com/</a>&quot;,<span style="color: rgb(255, 0, 0);">data</span> = data,headers=headers)</div><div>    data 的形式：字典</div><div>（3）举例：以有道翻译为例</div><div>       首先要在Network中找到<span style="color: rgb(255, 0, 0);">同时</span>包含输入数据和得到的结果的数据包。(找文件的技巧是自动跳过js、css以及图片文件，因为这些文件必然不是我们需要的数据包，然后可以看一下剩下的文件名，一般文件名会跟结果有些联系。接着，在Response和Preview中看看能不能同时看到输入数据和结果，若能看到，则该数据包必然是我们需要的)</div><div><img src="day1-爬虫基础和requests_files/Image [14].png" type="image/png" data-filename="Image.png"/></div><div>    然后便是找到响应的需要的值来写代码:</div><div><img src="day1-爬虫基础和requests_files/Image [15].png" type="image/png" data-filename="Image.png" width="916"/></div><div><br/></div><div>        可以看见，PC端浏览器下From Data有很多字段，很难判断哪些有用哪些没用，就算判断出来，也要花不好时间，并且，由于很多字段在Elements搜不到，是用js生成的，所以处理起来会比较麻烦。这个时候，可以换一种思路，将User-Agent换成手机端或者平板端，说不定会有意想不到的结果，使得需要处理的数据大大减少，以下就是手机端得到的数据包：</div><div><img src="day1-爬虫基础和requests_files/Image [16].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div>以下是在User-Agent为<span style="color: rgb(255, 0, 0);">手机端时</span>的post请求获取网页：</div><div><img src="day1-爬虫基础和requests_files/Image [17].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div><span style="font-weight: bold;">8、使用代理IP</span></div><div>    （1）为什么爬虫需要使用代理？</div><div>    • 让服务器以为不是同一个客户端在请求</div><div>    • 防止我们的真实地址被泄露，防止被追究</div><div>    （2）用法：requests.get(&quot;<a href="http://www.baidu.com/">http://www.baidu.com</a>&quot;, <span style="color: rgb(255, 0, 0);">proxies = proxies</span>)</div><div>        proxies的形式：字典</div><div>        proxies = {</div><div>        &quot;http&quot;: &quot;<a href="http://12.34.56.79:9527/">http://12.34.56.79:9527</a>&quot;,                            <span style="color: rgb(168, 168, 168);">#proxies字典里面一般只放一个，但也可以放两个，</span></div><div>        &quot;https&quot;: &quot;<a href="https://12.34.56.79:9527/">https://12.34.56.79:9527</a>&quot;,                         <span style="color: rgb(168, 168, 168);">#因为是字典，几个字段都行，无所谓</span></div><div>           }</div><div>        一般来说请求的是http的url，那么就放http的代理IP地址，如果请求的是https的url，就放https的代理IP</div><div><span style="font-weight: bold;"><img src="day1-爬虫基础和requests_files/Image [18].png" type="image/png" data-filename="Image.png"/></span></div><div>     （3）使用代理IP的思路：</div><div><span style="font-weight: bold;"><img src="day1-爬虫基础和requests_files/Image [19].png" type="image/png" data-filename="Image.png"/></span></div><div><br/></div><div><span style="font-weight: bold;">9、爬取登录后页面的方法</span></div><div><span style="font-weight: bold;">        </span>带上cookie的好处 ：</div><div>                能够请求到登录之后的页面</div><div>        带上cookie的弊端：</div><div>                一套cookie和session往往和一个用户对应</div><div>                请求太快，请求次数太多，容易被服务器识别为爬虫</div><div>        不需要cookie的时候尽量不去使用cookie</div><div>        但是为了获取登录之后的页面，我们必须发送带有cookie的请求</div><div><br/></div><div>requests 提供了一个叫做<span style="color: rgb(255, 0, 0);">session</span>类（<span style="color: rgb(255, 0, 0);">注意这里的session是requests的一个类，而不是服务器的保存的那个</span></div><div><span style="color: rgb(255, 0, 0);">                      session的意思</span>），来实现客户端和服务端的会话保持</div><div>使用方法：</div><div>    1. 实例化一个session对象</div><div>            session = requests.session()</div><div>    2. 让session发送get或者post请求</div><div>            response = session.get(url,headers)</div><div>            具体代码如下：</div><div><br/></div><div><span style="color: rgb(255, 0, 0);">        方法一，实例化requests中的session类，自己输入用户名和密码来登录，使session实例获得登录后的cookie，进而访问登录后的页面：</span></div><div><img src="day1-爬虫基础和requests_files/Image [20].png" type="image/png" data-filename="Image.png"/></div><div><img src="day1-爬虫基础和requests_files/Image [21].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div><img src="day1-爬虫基础和requests_files/Image [22].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div>        <span style="color: rgb(255, 0, 0);">方法二，复制登录成功后请求头里的cookie，直接放在headers里用requests.get（）访问登录后的页面：</span></div><div><img src="day1-爬虫基础和requests_files/Image [23].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div>     <span style="color: rgb(255, 0, 0);">方法三：也是</span><span style="color: rgb(255, 0, 0);">复制登录成功后请求头里的cookie，但是是将它再切分处理成字典形式，</span><span style="color: rgb(255, 0, 0);">传入</span><span style="color: rgb(255, 0, 0);">requests.get（）参数cookies里，而不放在</span><span style="color: rgb(255, 0, 0);">requests.get（）的参数</span><span style="color: rgb(255, 0, 0);">headers里面</span>（这方法比第二种略麻烦，第二种是最快的，尽可能使用第二种）</div><div><img src="day1-爬虫基础和requests_files/Image [24].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div>总结：</div><div><img src="day1-爬虫基础和requests_files/Image [25].png" type="image/png" data-filename="Image.png"/></div><div><img src="day1-爬虫基础和requests_files/Image [26].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div><span style="font-weight: bold;">10、字典推导式，列表推导式</span></div><div><img src="day1-爬虫基础和requests_files/Image [27].png" type="image/png" data-filename="Image.png"/></div><div><img src="day1-爬虫基础和requests_files/Image [28].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div><span style="font-weight: bold;">11、requests小技巧</span></div><div>    （1）cookie与url编解码</div><div><span style="font-weight: bold;"><img src="day1-爬虫基础和requests_files/Image [29].png" type="image/png" data-filename="Image.png"/></span></div><div>2、 请求SSL 证书验证</div><div>response = requests.get(&quot;<a href="https://www.12306.cn/mormhweb/">https://www.12306.cn/mormhweb/</a> &quot;, verify=False)</div><div>3、设置超时</div><div>response = requests.get(url,1)</div><div>4、配合状态码判断是否请求成功</div><div>assert response.status_code== 200</div></span>
</div></body></html> 