<html>
<head>
  <title>记录适配Databox的过程</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/603932 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 12pt;
    }
  </style>
</head>
<body>
<a name="6497"/>
<h1>记录适配Databox的过程</h1>
<div>
<table bgcolor="#D4DDE5" border="0">
<tr><td><b>创建时间：</b></td><td><i>2021/1/21 10:41</i></td></tr>
<tr><td><b>更新时间：</b></td><td><i>2021/4/15 15:59</i></td></tr>
</table>
</div>
<br/>

<div>
<span><div><b><a href="记录适配Databox的过程_files/Tez项目交接文档.docx"><img src="记录适配Databox的过程_files/ded3f071873bbed8892f6e658b1e2cd8.png" alt="Tez项目交接文档.docx"></a></b></div><div><span style="font-weight: bold;"><br/></span></div><div><span style="font-weight: bold;">在DATABOX芯片已经在集群的各个节点里</span><span style="font-weight: bold;">安装好</span><span style="font-weight: bold;">, 并且芯片的驱动没问题的情况下,并且将已经写好的DATABOX格式压缩算法的jar包放到hadoop安装目录/usr/lib/hadoop/lib下:</span></div><div>1、进入hive,  使用test数据库: use test;</div><div>执行如下命令:</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>create table xiaochuan_parquet_databox_mr stored as parquet tblproperties(&quot;parquet.compression&quot;=&quot;DATABOX&quot;) as select * from dns_text_100m_none;</div></div><div>报错如下: </div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>hive&gt; create table xiaochuan_parquet_databox_mr stored as parquet tblproperties(&quot;parquet.compression&quot;=&quot;DATABOX&quot;) as select * from dns_text_100m_none;</div><div>Query ID = root_20210121180141_b2c3f0e3-f976-4090-856d-c24df6e63bd0</div><div>Total jobs = 3</div><div>Launching Job 1 out of 3</div><div>Number of reduce tasks is set to 0 since there's no reduce operator</div><div>Starting Job = job_1611222110583_0002, Tracking URL = http://namenode:8088/proxy/application_1611222110583_0002/</div><div>Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1611222110583_0002</div><div>Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</div><div>2021-01-21 18:01:48,828 Stage-1 map = 0%,  reduce = 0%</div><div>2021-01-21 18:02:19,076 Stage-1 map = 100%,  reduce = 0%</div><div>Ended Job = job_1611222110583_0002 with errors</div><div>Error during job, obtaining debugging information...</div><div>Examining task ID: task_1611222110583_0002_m_000000 (and more) from job job_1611222110583_0002</div><div><br/></div><div><br/></div><div>Task with the most failures(4):</div><div>-----</div><div>Task ID:</div><div>  task_1611222110583_0002_m_000000</div><div><br/></div><div><br/></div><div>URL:</div><div>  http://namenode:8088/taskdetails.jsp?jobid=job_1611222110583_0002&amp;tipid=task_1611222110583_0002_m_000000</div><div>-----</div><div>Diagnostic Messages for this Task:</div><div>Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {&quot;length&quot;:&quot;1&quot;,&quot;local_province&quot;:&quot;471&quot;,&quot;local_city&quot;:&quot;0483&quot;,&quot;owner_province&quot;:&quot;471&quot;,&quot;owner_city&quot;:&quot;0471&quot;,&quot;roam_type&quot;:&quot;4&quot;,&quot;interface&quot;:&quot;11&quot;,&quot;xdr_id&quot;:&quot;593f8a7d000000cf0818cf0015173792&quot;,&quot;rat&quot;:&quot;6&quot;,&quot;imsi&quot;:&quot;460022484228323&quot;,&quot;imei&quot;:&quot;8695520280189378&quot;,&quot;msisdn&quot;:&quot;8618447303769&quot;,&quot;machine_ip_add_type&quot;:&quot;1&quot;,&quot;sgw_ggsn_ip_add&quot;:&quot;100.88.175.51&quot;,&quot;enb_sgsn_ip_add&quot;:&quot;100.19.164.121&quot;,&quot;pgw_add&quot;:&quot;&quot;,&quot;sgw_ggsn_port&quot;:&quot;0&quot;,&quot;enb_sgsn_port&quot;:&quot;0&quot;,&quot;pgw_port&quot;:&quot;65535&quot;,&quot;enb_sgsn_gtp_teid&quot;:&quot;2809&quot;,&quot;sgw_ggsn_gtp_teid&quot;:&quot;197188015&quot;,&quot;tac&quot;:&quot;18291&quot;,&quot;eci&quot;:&quot;80465677&quot;,&quot;apn&quot;:&quot;tmcan.ncb002.hkk460.rpqa&quot;,&quot;app_type_code&quot;:&quot;101&quot;,&quot;procedure_start_time&quot;:&quot;1497336480739538&quot;,&quot;procedure_end_time&quot;:&quot;1497336481186701&quot;,&quot;longitude&quot;:&quot;0&quot;,&quot;latitude&quot;:&quot;49.453910&quot;,&quot;protocol_type&quot;:&quot;4&quot;,&quot;app_type&quot;:&quot;18&quot;,&quot;app_sub_type&quot;:&quot;30944&quot;,&quot;app_content&quot;:&quot;0&quot;,&quot;app_status&quot;:&quot;255&quot;,&quot;ip_address_type&quot;:&quot;1&quot;,&quot;user_ipv4&quot;:&quot;10.213.47.85&quot;,&quot;user_ipv6&quot;:&quot;255.255.255.255&quot;,&quot;user_port&quot;:&quot;49501&quot;,&quot;l4_protocal&quot;:&quot;1&quot;,&quot;app_server_ip_ipv4&quot;:&quot;211.107.222.2&quot;,&quot;app_server_ip_ipv6&quot;:&quot;255.255.255.255&quot;,&quot;app_server_port&quot;:&quot;53&quot;,&quot;ul_data&quot;:&quot;98&quot;,&quot;dl_data&quot;:&quot;151&quot;,&quot;ul_ip_packet&quot;:&quot;1&quot;,&quot;dl_ip_packet&quot;:&quot;1&quot;,&quot;ul_dura&quot;:&quot;0&quot;,&quot;dl_dura&quot;:&quot;0&quot;,&quot;ul_disorder_ip_packet&quot;:&quot;0&quot;,&quot;dl_disorder_ip_packet&quot;:&quot;0&quot;,&quot;ul_retrans_ip_packet&quot;:&quot;0&quot;,&quot;dl_retrans_ip_packet&quot;:&quot;0&quot;,&quot;tcp_response_time&quot;:&quot;0&quot;,&quot;tcp_ack_time&quot;:&quot;0&quot;,&quot;ul_ip_frag_packets&quot;:&quot;0&quot;,&quot;dl_ip_frag_packets&quot;:&quot;0&quot;,&quot;first_req_time&quot;:&quot;0&quot;,&quot;first_response_time&quot;:&quot;0&quot;,&quot;window_size&quot;:&quot;0&quot;,&quot;mss&quot;:&quot;0&quot;,&quot;tcp_syn_num&quot;:&quot;0&quot;,&quot;tcp_status&quot;:&quot;1&quot;,&quot;session_end&quot;:&quot;2&quot;,&quot;tcp_syn_ack_mum&quot;:&quot;0&quot;,&quot;tcp_ack_num&quot;:&quot;0&quot;,&quot;tcp1_2_handshake_status&quot;:&quot;1&quot;,&quot;tcp2_3_handshake_status&quot;:&quot;1&quot;,&quot;repetition&quot;:&quot;0&quot;,&quot;domain_name&quot;:&quot;adashbc.m.taobao.com&quot;,&quot;ip_addr_num&quot;:&quot;2&quot;,&quot;ip_addr&quot;:&quot;104&quot;,&quot;rcode&quot;:&quot;111.184.179.26,79.13.221.100,111.214.122.99,158.13.179.187&quot;,&quot;dnsreq_num&quot;:&quot;0&quot;,&quot;ancount&quot;:&quot;1&quot;,&quot;nscount&quot;:&quot;2&quot;,&quot;arcount&quot;:&quot;0&quot;,&quot;response_time&quot;:&quot;0&quot;}</div><div>        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:172)</div><div>        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)</div><div>        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)</div><div>        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)</div><div>        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:175)</div><div>        at java.security.AccessController.doPrivileged(Native Method)</div><div>        at javax.security.auth.Subject.doAs(Subject.java:422)</div><div>        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1961)</div><div>        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:169)</div><div>Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {&quot;length&quot;:&quot;1&quot;,&quot;local_province&quot;:&quot;471&quot;,&quot;local_city&quot;:&quot;0483&quot;,&quot;owner_province&quot;:&quot;471&quot;,&quot;owner_city&quot;:&quot;0471&quot;,&quot;roam_type&quot;:&quot;4&quot;,&quot;interface&quot;:&quot;11&quot;,&quot;xdr_id&quot;:&quot;593f8a7d000000cf0818cf0015173792&quot;,&quot;rat&quot;:&quot;6&quot;,&quot;imsi&quot;:&quot;460022484228323&quot;,&quot;imei&quot;:&quot;8695520280189378&quot;,&quot;msisdn&quot;:&quot;8618447303769&quot;,&quot;machine_ip_add_type&quot;:&quot;1&quot;,&quot;sgw_ggsn_ip_add&quot;:&quot;100.88.175.51&quot;,&quot;enb_sgsn_ip_add&quot;:&quot;100.19.164.121&quot;,&quot;pgw_add&quot;:&quot;&quot;,&quot;sgw_ggsn_port&quot;:&quot;0&quot;,&quot;enb_sgsn_port&quot;:&quot;0&quot;,&quot;pgw_port&quot;:&quot;65535&quot;,&quot;enb_sgsn_gtp_teid&quot;:&quot;2809&quot;,&quot;sgw_ggsn_gtp_teid&quot;:&quot;197188015&quot;,&quot;tac&quot;:&quot;18291&quot;,&quot;eci&quot;:&quot;80465677&quot;,&quot;apn&quot;:&quot;tmcan.ncb002.hkk460.rpqa&quot;,&quot;app_type_code&quot;:&quot;101&quot;,&quot;procedure_start_time&quot;:&quot;1497336480739538&quot;,&quot;procedure_end_time&quot;:&quot;1497336481186701&quot;,&quot;longitude&quot;:&quot;0&quot;,&quot;latitude&quot;:&quot;49.453910&quot;,&quot;protocol_type&quot;:&quot;4&quot;,&quot;app_type&quot;:&quot;18&quot;,&quot;app_sub_type&quot;:&quot;30944&quot;,&quot;app_content&quot;:&quot;0&quot;,&quot;app_status&quot;:&quot;255&quot;,&quot;ip_address_type&quot;:&quot;1&quot;,&quot;user_ipv4&quot;:&quot;10.213.47.85&quot;,&quot;user_ipv6&quot;:&quot;255.255.255.255&quot;,&quot;user_port&quot;:&quot;49501&quot;,&quot;l4_protocal&quot;:&quot;1&quot;,&quot;app_server_ip_ipv4&quot;:&quot;211.107.222.2&quot;,&quot;app_server_ip_ipv6&quot;:&quot;255.255.255.255&quot;,&quot;app_server_port&quot;:&quot;53&quot;,&quot;ul_data&quot;:&quot;98&quot;,&quot;dl_data&quot;:&quot;151&quot;,&quot;ul_ip_packet&quot;:&quot;1&quot;,&quot;dl_ip_packet&quot;:&quot;1&quot;,&quot;ul_dura&quot;:&quot;0&quot;,&quot;dl_dura&quot;:&quot;0&quot;,&quot;ul_disorder_ip_packet&quot;:&quot;0&quot;,&quot;dl_disorder_ip_packet&quot;:&quot;0&quot;,&quot;ul_retrans_ip_packet&quot;:&quot;0&quot;,&quot;dl_retrans_ip_packet&quot;:&quot;0&quot;,&quot;tcp_response_time&quot;:&quot;0&quot;,&quot;tcp_ack_time&quot;:&quot;0&quot;,&quot;ul_ip_frag_packets&quot;:&quot;0&quot;,&quot;dl_ip_frag_packets&quot;:&quot;0&quot;,&quot;first_req_time&quot;:&quot;0&quot;,&quot;first_response_time&quot;:&quot;0&quot;,&quot;window_size&quot;:&quot;0&quot;,&quot;mss&quot;:&quot;0&quot;,&quot;tcp_syn_num&quot;:&quot;0&quot;,&quot;tcp_status&quot;:&quot;1&quot;,&quot;session_end&quot;:&quot;2&quot;,&quot;tcp_syn_ack_mum&quot;:&quot;0&quot;,&quot;tcp_ack_num&quot;:&quot;0&quot;,&quot;tcp1_2_handshake_status&quot;:&quot;1&quot;,&quot;tcp2_3_handshake_status&quot;:&quot;1&quot;,&quot;repetition&quot;:&quot;0&quot;,&quot;domain_name&quot;:&quot;adashbc.m.taobao.com&quot;,&quot;ip_addr_num&quot;:&quot;2&quot;,&quot;ip_addr&quot;:&quot;104&quot;,&quot;rcode&quot;:&quot;111.184.179.26,79.13.221.100,111.214.122.99,158.13.179.187&quot;,&quot;dnsreq_num&quot;:&quot;0&quot;,&quot;ancount&quot;:&quot;1&quot;,&quot;nscount&quot;:&quot;2&quot;,&quot;arcount&quot;:&quot;0&quot;,&quot;response_time&quot;:&quot;0&quot;}</div><div>        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:518)</div><div>        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:163)</div><div>        ... 8 more</div><div>Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: <font color="#FF0000">No enum constant parquet.hadoop.metadata.CompressionCodecName.DATABOX</font></div><div>        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:577)</div><div>        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:675)</div><div>        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:842)</div><div>        at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)</div><div>        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:842)</div><div>        at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:97)</div><div>        at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:162)</div><div>        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:508)</div><div>        ... 9 more</div><div>Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: <font color="#FF0000">N</font><font color="#FF0000">o enum constant parquet.hadoop.metadata.CompressionCodecName.DATABOX</font></div><div>        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:249)</div><div>        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketForFileIdx(FileSinkOperator.java:622)</div><div>        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:566)</div><div>        ... 16 more</div><div>Caused by: java.lang.IllegalArgumentException: No enum constant parquet.hadoop.metadata.CompressionCodecName.DATABOX</div><div>        at java.lang.Enum.valueOf(Enum.java:238)</div><div>        at parquet.hadoop.metadata.CompressionCodecName.valueOf(CompressionCodecName.java:24)</div><div>        at parquet.hadoop.metadata.CompressionCodecName.fromConf(CompressionCodecName.java:34)</div><div>        at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.initializeSerProperties(ParquetRecordWriterWrapper.java:94)</div><div>        at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.&lt;init&gt;(ParquetRecordWriterWrapper.java:61)</div><div>        at org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.getParquerRecordWriterWrapper(MapredParquetOutputFormat.java:125)</div><div>        at org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.getHiveRecordWriter(MapredParquetOutputFormat.java:114)</div><div>        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:261)</div><div>        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:246)</div><div>        ... 18 more</div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div>FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask</div><div>MapReduce Jobs Launched:</div><div>Stage-Stage-1: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL</div><div>Total MapReduce CPU Time Spent: 0 msec</div></div><div>可以发现问题是缺少了&quot;DATABOX&quot;枚举。所以想到需要修改源码, 把&quot;DATABOX&quot;枚举添加到hive或parquet的某部分源码里, 然后打成jar包替换掉公司的部分jar包, 让&quot;DATABOX&quot;枚举可以顺利在hive中找到, 然后通过反射找到我们公司的DATABOX的压缩jar包。</div><div>首先需要下载hive的源码包以便于修改:</div><div><span style="font-size: unset; color: unset; font-family: unset;">    通过whereis hive找到hive安装包的位置, 找到 hive的lib库目录在/usr/lib/hive/lib下, 通过hive-common-1.2.2.jar了解到hive的版本为1.2.2。</span><span style="font-size: unset; color: unset; font-family: unset;">在github中搜索&quot;hive&quot;,第一个就是Apache官方的源码路径。通过tag选项找到release-1.2.2版本, 然后直接下载压缩包。 (注意, 最好不要用</span><span style="font-size: unset; color: unset; font-family: unset;">git clone +链接的方式git源码, 因为这样git的是所有版本的hive的源码(除非加上参数指定版本号), 要下非常久,下压缩包的话只是对应的版本的源码, 比较方便</span><span style="font-size: unset; color: unset; font-family: unset;">)</span></div><div><img src="记录适配Databox的过程_files/Image.png" type="image/png" data-filename="Image.png" width="983"/></div><div><br/></div><div>    下载完源码后 ,解压, 最好在Linux里编译, windows里编译容易有各种问题。可以先在Linux里编译一次, 编译是cd到解压后的源码包的一级目录(统领所有子项目的pom.xml所在的目录), 执行<span style="box-sizing: border-box; border: none; font-size: 14px; color: rgb(0, 0, 0); font-family: Arial; font-stretch: normal; font-variant: normal; line-height: 21px;">mvn clean package -Phadoop-2 -DskipTests -Pdist </span> 进行编译。具体hive的其他编译可参考hive笔记本</div><div>源码直接编译后的输出结果如下, 会生成很多个文件, 其中编译产生的apache-hive-1.2.2-src.tar.gz和github下载的源码包hive-rel-release-1.2.2.zip是一样的内容, apache-hive-1.2.2-bin也只是apache-hive-1.2.2-bin.tar.gz的解压后的结果而已, 但这些结果都会由编译产生。最重要的只是apache-hive-1.2.2-bin.tar.gz这个jar包。(当然, 编译hive源码的目的只是为了看看编译是否正常, 实际上并不一定需要编译, 我们只是需要去源码里找<a href="http://parquet.hadoop.metadata.compressioncodecname/">parquet.hadoop.metadata.CompressionCodecName</a>这个类)</div><div><img src="记录适配Databox的过程_files/Image [1].png" type="image/png" data-filename="Image.png"/></div><div>如果编译没有什么问题, 再对源码包里的源码进行修改, 源码的修改可以放到Windows里的IDE里面, 可视化较好。或者如果Linux有图形化界面, 用Linux也可以。</div><div><br/></div><div>对源码进行修改:</div><div>    首先找到上面的报错提示所在的类parquet.hadoop.metadata.CompressionCodecName, 找到后,观察其路径发现它并不是hive源码里的类, 而是hive的依赖包parquet-hadoop-bundle-1.6.0.jar包里的类, 所以接下来就有了两种思路:</div><div><span style="font-weight: bold;">第一种方法:</span></div><div>首先想到比较简单的方式就是在Linux里面创建要修改的类CompressionCodecName为一个文件,以.java结尾(直接用vim CompressionCodecName<span style="color: rgb(255, 0, 0);">.java</span> ),然后将刚刚在IDE里找到的<a href="http://parquet.hadoop.metadata.compressioncodecname/">parquet.hadoop.metadata.CompressionCodecName</a>类的所有源代码从IDE中复制粘贴进去, 进行相应的修改, 然后保存退出。如下图:</div><div><img src="记录适配Databox的过程_files/Image [2].png" type="image/png" data-filename="Image.png"/>弄好后直接去刚刚编译好的hive源码包的target目录下, 找到编译好的可执行jar包的可执行包(apache-hive-1.2.2-bin)的lib目录里找该jar包(注意不要直接在公司安装的hive的lib包里找, 因为该lib可能被修改过), 因为既然hive源码包依赖于parquet-hadoop-bundle-1.6.0.jar包, 那么在lib库里肯定有该jar包, 将修改好的类和parquet-hadoop-bundle-1.6.0.jar放在同一个目录下。修改完后直接用javac编译:</div><div>    javac -classpath parquet-hadoop-bundle-1.6.0.jar 修改了的类1 修改了的类2 修改了的类N。</div><div>    一次即可将所有修改完了的.java文件编译完成, 生成对应的.class文件。一次将所有的类编译也解决了这些修改了的类之间相互有依赖的问题, 直接将这些类替换掉jar包中对应的类编译。</div><div>    </div><div><br/></div><div><span style="font-weight: bold;">第二种方法:</span> </div><div>    下载parquet对应版本的源码包, 首先编译一遍确保能成功编译, 然后修改对应要修改的类(比如下面这个类CompressionCodecName.java), 再将整个源码包编译一遍。整体编译的好处是安全, 对不太了解编译实际原理的人来说比较方便(我就是这样的人)。</div><div><img src="记录适配Databox的过程_files/Image [3].png" type="image/png" data-filename="Image.png"/></div><div>可以发现上图蓝框处的枚举有SNAPPY、GZIP和LZO, 此时只需要在后面添加一行DATABOX的枚举即可。同时还要发现在枚举中的绿框参数中, SNAPPY、GZIP和LZO都来自CompressionCodec类, 也就意味着CompressionCodec类里也要仿照SNAPPY、GZIP和LZO这三个加上一个相应的才行。</div><div>首先在CompressionCodecName类里修改, 在LZO后面添加一行代码:</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>DATABOX(&quot;<a href="http://com.hadoop.compression.databox.databoxcodec/">com.hadoop.compression.databox.DataboxCodec</a>&quot;, CompressionCodec.DATABOX, &quot;.dtbxzenc&quot;);</div></div><div>变为如下样式: </div><div><img src="记录适配Databox的过程_files/Image [4].png" type="image/png" data-filename="Image.png" width="1146"/></div><div>CompressionCodec类也在parquet-hadoop-bundle-1.6.0.jar包里:</div><div><img src="记录适配Databox的过程_files/Image [5].png" type="image/png" data-filename="Image.png" width="890"/></div><div>其次在CompressionCodec类里修改如下:</div><div><img src="记录适配Databox的过程_files/Image [6].png" type="image/png" data-filename="Image.png" width="845"/></div><div><br/></div><div><br/></div><div><span style="font-size: 9pt;"><br/></span></div><div>编译后需要改/usr/lib/parquet/lib的三个jar包, 分别是</div><div>    parquet-format-2.1.0-cdh5.4.0.jar</div><div>    parquet-hadoop-1.5.0-cdh5.4.0.jar</div><div>    parquet-hadoop-bundle-1.5.0-cdh5.4.0.jar</div><div><br/></div><div>将jar包分发到所有集群节点的hadoop/lib和hive/lib目录下, 并且把芯片需要的jar包分发到hadoop/lib和hive/lib目录下。</div><div>发现不再报上面的错误, 但是只在219节点上有压缩速度, 而218上没有, (217是主节点没有设为<span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">DataNode和</span> <span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">NodeManager, </span>所以没有压缩速度正常), 而且建表时表的数据量过大也会报错, 是218引起的, 结果发现只要把218的 <span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">DataNode和</span> <span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">NodeManager</span>上面的服务关闭, 那么无论是建表是大数据量还是小数据量都能成功, 但是有一个问题是不能解压。执行select * from xiaochuan_parquet_databox_mr limit 10; </div><div>只能写,不能读。报错如下:</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>hive&gt; select * from xiaochuan_parquet_databox_mr limit 10;</div><div>OK</div><div>Failed with exception java.io.IOException:java.io.IOException: can not read class parquet.format.FileMetaData: Required field 'codec' was not present! Struct: ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, PLAIN, BIT_PACKED], path_in_schema:[length], codec:null, num_values:900040, total_uncompressed_size:8556423, total_compressed_size:1115094, data_page_offset:4, statistics:Statistics(max:39 39 39 39 39, min:31, null_count:0))</div><div>Time taken: 0.119 seconds</div></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div>依次将压缩jar包hadoop-2.6.0-databox-aries-v1-1.0.0.jar放到</div><div>    /usr/lib/hadoop-hdfs/lib</div><div>    /usr/lib/hadoop/lib </div><div>    /usr/lib/hive/lib</div><div>    依然无效</div><div>(这是由于没有把改过的parquetjar包分发到各个节点)</div><div><br/></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>Query ID = root_20210121103024_fa417722-d1de-4556-af81-0214886f41ea</div><div>Total jobs = 3</div><div>Launching Job 1 out of 3</div><div>Number of reduce tasks is set to 0 since there's no reduce operator</div><div>Starting Job = job_1611190961543_0003, Tracking URL = http://namenode:8088/proxy/application_1611190961543_0003/</div><div>Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1611190961543_0003</div><div>Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</div><div>2021-01-21 10:30:31,275 Stage-1 map = 0%,  reduce = 0%</div><div>2021-01-21 10:31:05,733 Stage-1 map = 100%,  reduce = 0%</div><div>Ended Job = job_1611190961543_0003 with errors</div><div>Error during job, obtaining debugging information...</div><div>Examining task ID: task_1611190961543_0003_m_000000 (and more) from job job_1611190961543_0003</div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div>Task with the most failures(4):</div><div>-----</div><div>Task ID:</div><div>  task_1611190961543_0003_m_000000</div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div>URL:</div><div>  http://namenode:8088/taskdetails.jsp?jobid=job_1611190961543_0003&amp;tipid=task_1611190961543_0003_m_000000</div><div>-----</div><div>Diagnostic Messages for this Task:</div><div>Error: java.io.IOException: java.lang.reflect.InvocationTargetException</div><div>        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)</div><div>        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)</div><div>        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:266)</div><div>        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.&lt;init&gt;(HadoopShimsSecure.java:213)</div><div>        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:333)</div><div>        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:720)</div><div>        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.&lt;init&gt;(MapTask.java:169)</div><div>        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)</div><div>        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)</div><div>        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:175)</div><div>        at java.security.AccessController.doPrivileged(Native Method)</div><div>        at javax.security.auth.Subject.doAs(Subject.java:422)</div><div>        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1961)</div><div>        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:169)</div><div>Caused by: java.lang.reflect.InvocationTargetException</div><div>        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div>        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</div><div>        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div>        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div>        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:252)</div><div>        ... 11 more</div><div>Caused by: java.io.IOException: Cannot create an instance of InputFormat class org.apache.hadoop.mapred.TextInputFormat as specified in mapredWork!</div><div>        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(HiveInputFormat.java:212)</div><div>        at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.&lt;init&gt;(CombineHiveRecordReader.java:59)</div><div>        ... 16 more</div><div>Caused by: java.lang.RuntimeException: Error in configuring object</div><div>        at org.apache.hive.common.util.ReflectionUtil.setJobConf(ReflectionUtil.java:115)</div><div>        at org.apache.hive.common.util.ReflectionUtil.setConf(ReflectionUtil.java:103)</div><div>        at org.apache.hive.common.util.ReflectionUtil.newInstance(ReflectionUtil.java:87)</div><div>        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(HiveInputFormat.java:204)</div><div>        ... 17 more</div><div>Caused by: java.lang.reflect.InvocationTargetException</div><div>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div><div>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</div><div>        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div>        at java.lang.reflect.Method.invoke(Method.java:498)</div><div>        at org.apache.hive.common.util.ReflectionUtil.setJobConf(ReflectionUtil.java:112)</div><div>        ... 20 more</div><div>Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.databox.DataboxCodec not found.</div><div>        at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:139)</div><div>        at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:180)</div><div>        at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)</div><div>        ... 25 more</div><div>Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.databox.DataboxCodec not found</div><div>        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2202)</div><div>        at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:132)</div><div>        ... 27 more</div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div>FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask</div><div>MapReduce Jobs Launched:</div><div>Stage-Stage-1: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL</div><div>Total MapReduce CPU Time Spent: 0 msec  </div></div><div><span style="font-size: unset; color: unset; font-family: unset;">  </span></div><div><span style="font-size: unset;"><br/></span></div><div><img src="记录适配Databox的过程_files/Image [7].png" type="image/png" data-filename="Image.png"/></div><div><span style="font-size: unset;"><br/></span></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>hive&gt; select * from xiaochuan_parquet_databox_mr4 limit 10;</div><div>OK</div><div>Failed with exception java.io.IOException:java.io.IOException: can not read class parquet.format.FileMetaData: Required field 'codec' was not present! Struct: ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, PLAIN, BIT_PACKED], path_in_schema:[length], codec:null, num_values:225010, total_uncompressed_size:2139141, total_compressed_size:279571, data_page_offset:4, statistics:Statistics(max:39 39 39 39 39, min:31, null_count:0))</div><div>Time taken: 0.166 seconds</div></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>hive&gt; select count(*) from xiaochuan_parquet_databox_mr4</div><div>    &gt; ;</div><div>OK</div><div>225010</div><div>Time taken: 0.167 seconds, Fetched: 1 row(s)</div></div><div>---------------------------------------------------------------------------------------------------------------------------</div><div><br/></div><div><br/></div><div>1、修改parquet-format-2.1.0</div><div>下载源码parquet-format-2.1.0, 修改文件src/thrift/parquet.thrift中的CompressionCodec枚举。</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>enum CompressionCodec {</div><div>  UNCOMPRESSED = 0;</div><div>  SNAPPY = 1;</div><div>  GZIP = 2;</div><div>  LZO = 3;</div><div>  <font color="#FF0000">DATABOX = 4;</font></div><div>}</div></div><div>在本地Linux环境中安装thrift, 并将parquet-format-2.1.0项目里的pom.xml中的如下依赖的thrift version改为与本地环境安装的thrift的版本一致。</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>    &lt;dependency&gt;</div><div>      &lt;groupId&gt;org.apache.thrift&lt;/groupId&gt;</div><div>      &lt;artifactId&gt;libthrift&lt;/artifactId&gt;</div><div>      &lt;version&gt;<font color="#FF0000">0.9.1</font>&lt;/version&gt;</div><div>    &lt;/dependency&gt;</div></div><div>接下来的步骤有两种做法:</div><div>第一种做法: </div><div>    最简单的做法, 再将parquet-format-2.1.0项目里的pom.xml<span style="font-size: unset; color: unset; font-family: unset;">的</span><span style="font-size: unset; color: unset; font-family: unset;">maven-thrift-plugin插件的版本由0.1.10改为0.1.11(或更高版本), 因为现在</span><span style="font-size: unset; color: unset; font-family: unset;">maven-thrift-plugin在各个maven中央仓库中的最低版本就是0.1.11, 如果保持为</span><span style="font-size: unset; color: unset; font-family: unset;">0.1.10,则用maven会始终编译不成功。</span></div><div>    <img src="记录适配Databox的过程_files/skitch.png" type="image/png" data-filename="skitch.png"/></div><div>    修改好后, 执行<span style="color: rgb(255, 0, 0); font-weight: bold;">mvn install -DskipTests</span>。即可生成parquet-format-2.1.0.jar。</div><div>    注意:<span style="color: rgb(255, 0, 0); font-weight: bold;"> 这里需要执行mvn install -DskipTests而不是</span><span style="color: rgb(255, 0, 0); font-weight: bold;">mvn package -DskipTests是</span><span style="color: rgb(255, 0, 0); font-weight: bold;">因为我们要将修改过的parquet-format-2.1.0.jar发布到本地仓库, 以供接下来的parquet-mr-parquet-1.5.0使用mvn编译时使用。</span></div><div>第二种做法:    </div><div>    使用thrift执行src/thrift/parquet.thrift文件,用thrift --gen java parquet.thrift ,(或者在项目的一级目录下直接执行make命令也可以, 因为Makefile文件里的就是thrift命令),生成parquet.format包下的所有代码。</div><div>    <img src="记录适配Databox的过程_files/Image [8].png" type="image/png" data-filename="Image.png"/></div><div>    并将生成好的所有java源码文件复制到parquet-format-2.1.0项目中的src/main/java/parquet/format目录下。然后将pom.xml文件中多余的内容删除, 只留下如下内容: (注意, 只剩下如下内容会使得最终编译出来的jar包比第一种方法的jar包少一部分源码, 所以还是推荐第一种方法, 因为第一种方法生成的jar包才是和maven中央仓库下载的源码数量一样)</div><div>    <img src="记录适配Databox的过程_files/Image [9].png" type="image/png" data-filename="Image.png"/></div><div>    执行mvn package -DskipTests。生成parquet-format-2.1.0.jar。</div><div><br/></div><div>(2)、下载源码parquet-mr-parquet-1.5.0, 修改文件parquet-mr-parquet-1.5.0/parquet-hadoop/src/main/java/parquet/hadoop/metadata/CompressionCodecName.java文件</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>DATABOX(&quot;com.hadoop.compression.databox.DataboxCodec&quot;, CompressionCodec.DATABOX, &quot;.dtbxzenc&quot;);</div></div><div>接下来</div><div>方法一:  按照(1)中的方法一, 并且把parquet-mr-parquet-1.5.0/pom.xml里面的以下内容注释掉:</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>     &lt;module&gt;parquet-cascading&lt;/module&gt;</div><div>     &lt;module&gt;parquet-protobuf&lt;/module&gt;</div><div>     &lt;module&gt;parquet-scrooge&lt;/module&gt;</div><div>     &lt;module&gt;parquet-thrift&lt;/module&gt;</div><div>     &lt;module&gt;parquet-hive&lt;/module&gt;</div><div>     &lt;module&gt;parquet-hive-bundle&lt;/module&gt;</div></div><div>则可直接编译源码parquet-mr-parquet-1.5.0</div><div>方法二: 按照(1)中的方法二, 则需要:</div><div>    将(1)中编译好的parquet-format-2.1.0.jar解压。 找到文件parquet/format/CompressionCodec.class,  将其替换掉maven本地仓库中从中央仓库下载的parquet-format-2.1.0.jar包的同路径下的同名文件。并且把parquet-mr-parquet-1.5.0/pom.xml里面的以下内容注释掉:</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>     &lt;module&gt;parquet-cascading&lt;/module&gt;</div><div>     &lt;module&gt;parquet-protobuf&lt;/module&gt;</div><div>     &lt;module&gt;parquet-scrooge&lt;/module&gt;</div><div>     &lt;module&gt;parquet-thrift&lt;/module&gt;</div><div>     &lt;module&gt;parquet-hive&lt;/module&gt;</div><div>     &lt;module&gt;parquet-hive-bundle&lt;/module&gt;</div></div><div>再编译源码parquet-mr-parquet-1.5.0即可成功<span style="font-size: unset; color: unset; font-family: unset;">(否则将(1)中方法二编译好的</span>parquet-format-2.1.0.jar发送到本地仓库会导致编译不成功, 因为缺少内容<span style="font-size: unset; color: unset; font-family: unset;">)</span></div><div><br/></div><div><br/></div><div><br/></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>maven将jar包发布到本地仓库除了编译的时候install, 还可用以下命令, 直接install Linux文件目录中的jar包。如发布hadoop-2.6.0-databox-aries-v1-1.0.0.jar:</div><div>mvn install:install-file -Dfile=hadoop-2.6.0-databox-aries-v1-1.0.0.jar -DgroupId=com.hadoop.compression.databox -DartifactId=hadoop-2.6.0-databox-aries-v1 -Dversion=1.0.0 -Dpackaging=jar</div></div><div>发布后可在本地仓库中找打如下所示:</div><div><img src="记录适配Databox的过程_files/Image [10].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div><br/></div><div><img src="记录适配Databox的过程_files/Image [11].png" type="image/png" data-filename="Image.png"/></div><div>    &lt;dependency&gt;</div><div>      &lt;groupId&gt;com.twitter&lt;/groupId&gt;</div><div>      &lt;artifactId&gt;parquet-format&lt;/artifactId&gt;</div><div>      &lt;version&gt;${parquet.format.version}&lt;/version&gt;</div><div>    &lt;/dependency&gt;</div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><span style="color: rgb(166, 0, 196);">最简单的适配Databox的过程:</span></div><div><span style="color: rgb(166, 0, 196);">    经过上面的思考, 我们发现, 其实只需要替换四个jar包中的CompressionCodec.class和CompressionCodecName.class 两个文件即可。那么最简单的替换方法就是, 准备好l两个源码文件CompressionCodec.java和CompressionCodecName.java, 用javac -classpath 命令, 分别用那四个需要改动的,正在集群中正在使用的jar包将这两个源码文件编译, 当然并不是所有jar包这两个文件都需要, 有的只需要一个就够了, 那就只编译一个。所有的编译命令如下:</span></div><div><span style="color: rgb(166, 0, 196);">    javac -classpath parquet-format-2.1.0-cdh5.4.0.jar CompressionCodec.java</span></div><div><span style="color: rgb(166, 0, 196);">    javac -classpath parquet-hadoop-1.5.0-cdh5.4.0.jar CompressionCodecName.java</span></div><div><span style="color: rgb(166, 0, 196);">    javac -classpath parquet-hadoop-bundle-1.5.0-cdh5.4.0.jar CompressionCodec.java CompressionCodec.java</span></div><div><span style="color: rgb(166, 0, 196);">    javac -classpath hive-exec-1.2.2-bc1.4.0.jar CompressionCodec.java CompressionCodec.java</span></div><div><span style="color: rgb(166, 0, 196);">    然后再把各自编译好的.class文件替换掉各自jar包中的文件, 再打包。最后替换掉集群中原先使用的jar包即可。</span></div><div>    </div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">也就是</span></div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">1、解压/usr/lib/parquet/lib/parquet-hadoop-1.5.0-cdh5.4.0.jar </span></div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">        替换parquet/format/CompressionCodec.class</span></div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">2、解压/usr/lib/parquet/lib/parquet-hadoop-bundle-1.5.0-cdh5.4.0.jar</span></div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">        替换parquet/hadoop/metadata/CompressionCodecName.class</span></div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">        替换parquet/format/CompressionCodec.class</span></div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">3、解压/usr/lib/parquet/lib/parquet-hadoop-1.5.0-cdh5.4.0.jar</span></div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">        替换parquet/hadoop/metadata/CompressionCodecName.class</span></div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">4、解压/usr/lib/hive/lib/hive-exec-1.2.2-bc1.4.0.jar</span></div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">        替换parquet/format/CompressionCodec.class</span></div><div style="margin-left: 40px;"><span style="color: rgb(166, 0, 196);">        替换parquet/hadoop/metadata/CompressionCodecName.class</span></div></span>
</div></body></html> 